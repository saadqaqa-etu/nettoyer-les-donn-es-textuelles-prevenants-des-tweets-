{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14TY4LTBZOm9RT8xBC9csyNn90mXvo5BW",
      "authorship_tag": "ABX9TyOlM6IaujNr66lwRtN/CHuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadqaqa-etu/nettoyer-les-donn-es-textuelles-prevenants-des-tweets-/blob/main/untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCzT6CggCvNo",
        "outputId": "bb0a0904-0804-416a-e25d-c0daad3dcced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    # Supprimer les hashtags, les mentions et les URL √† l'aide d'expressions r√©guli√®res\n",
        "    tweet = re.sub(r'#\\w+', '', tweet)                  # Supprimer les hashtags\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)                  # Supprimer les mentions\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)               # Supprimer les URL\n",
        "\n",
        "    # Supprimer les emojis √† l'aide d'expressions r√©guli√®res\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # Emojis souriants\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # Symboles et ic√¥nes\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # Emojis de transport\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # Emojis d'alphabets\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Emojis de diversit√©\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Emojis de m√©t√©o\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Emojis de v√™tements\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Emojis d'objets\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Emojis de nourriture\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Emojis divers\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    tweet = emoji_pattern.sub(r'', tweet)              # Supprimer les emojis\n",
        "\n",
        "    # Convertir le tweet en minuscules pour l'uniformit√©\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Tokenization : diviser le tweet en mots individuels\n",
        "    words = word_tokenize(tweet)\n",
        "\n",
        "    # Supprimer les mots vides (stop words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Rejoindre les mots en un seul texte nettoy√©\n",
        "    cleaned_tweet = ' '.join(words)\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0zfqMu67C5-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple d'utilisation\n",
        "tweet = \"Just had a great time at the #beach with @friend! üòÉüèñÔ∏è #summerfun Check out the photos: http://example.com\"\n",
        "cleaned_tweet = clean_tweet(tweet)\n",
        "print(cleaned_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VHWhMgzC6-o",
        "outputId": "0e944441-585a-4b5d-a201-55fb77a5d18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great time ! check photos :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/train.json', 'r', encoding='utf-8') as file:\n",
        "    dataset = file.readlines()\n"
      ],
      "metadata": {
        "id": "4SOgK7n8C-ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_dataset = [clean_tweet(tweet) for tweet in dataset]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5xkzQS5bDC1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder_path = 'drive/MyDrive/'\n",
        "output_file_path = output_folder_path + 'cleaned_twibot20_dataset.json'\n",
        "\n",
        "# Save the cleaned dataset to the \"MyResults\" folder\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    for cleaned_tweet in cleaned_dataset:\n",
        "        file.write(cleaned_tweet + '\\n')\n"
      ],
      "metadata": {
        "id": "roTAStQiX5g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sKC94t8OS4Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fm3cYEOlSMAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qUFYA4FMSNCq"
      }
    }
  ]
}