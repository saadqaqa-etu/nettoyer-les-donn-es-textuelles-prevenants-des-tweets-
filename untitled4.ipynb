{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-S-2l__sDQDdSSC3G7QUQI6VmOS4gxO4",
      "authorship_tag": "ABX9TyPe7m6JJUKAGJatvlCHRlwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadqaqa-etu/nettoyer-les-donn-es-textuelles-prevenants-des-tweets-/blob/main/untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzc8AUwUFkx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfb4abc-2f88-452e-faf1-9952280d5c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    # Supprimer les hashtags, les mentions et les URL à l'aide d'expressions régulières\n",
        "    tweet = re.sub(r'#\\w+', '', tweet)                  # Supprimer les hashtags\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)                  # Supprimer les mentions\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)               # Supprimer les URL\n",
        "\n",
        "    # Supprimer les retweets\n",
        "    tweet = re.sub(r'RT @\\w+:', '', tweet)\n",
        "\n",
        " # Supprimer les emojis à l'aide d'expressions régulières\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # Emojis souriants\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # Symboles et icônes\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # Emojis de transport\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # Emojis d'alphabets\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Emojis de diversité\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Emojis de météo\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Emojis de vêtements\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Emojis d'objets\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Emojis de nourriture\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Emojis divers\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "\n",
        "    # Convertir le tweet en minuscules pour l'uniformité\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "  # Initialiser le stemmer et le lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenization : diviser le tweet en mots individuels\n",
        "    words = word_tokenize(tweet)\n",
        "\n",
        "    # Supprimer les mots vides (stop words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Stemming et lemmatisation\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "    # Rejoindre les mots en un seul texte nettoyé\n",
        "    cleaned_tweet = ' '.join(lemmatized_words)\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = '/train.json'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    dataset = file.readlines()\n",
        "\n",
        "# Apply the cleaning function to the dataset\n",
        "cleaned_dataset = [clean_tweet(tweet) for tweet in dataset]\n",
        "\n",
        "# Define the output file path within the \"MyResults\" folder\n",
        "output_folder_path = 'drive/MyDrive/'\n",
        "output_file_path = output_folder_path + 'cleaned_twibot20_dataset2.json'\n",
        "\n",
        "# Save the cleaned dataset to the \"MyResults\" folder\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    for cleaned_tweet in cleaned_dataset:\n",
        "        file.write(cleaned_tweet + '\\n')"
      ],
      "metadata": {
        "id": "RHM8f5p_MgPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "L1mI9NDJLxlU"
      }
    }
  ]
}